{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q1. Import the dataset and examine the variables\n",
    "To begin, we will load the dataset and explore it using descriptive statistics and visualizations. This helps us understand the distribution of the data and relationships between the variables.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Display summary statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Visualize the distribution of each feature\n",
    "df.hist(figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the correlation matrix to identify relationships\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the distribution of the target variable (Outcome)\n",
    "sns.countplot(x='Outcome', data=df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Preprocess the data\n",
    "# Before training the model, we need to preprocess the data:\n",
    "\n",
    "# Handle missing values: Some columns may have missing values that need to be filled or removed.\n",
    "# Remove outliers: Outliers may affect model performance.\n",
    "# Transform categorical variables: In this dataset, the \"Outcome\" variable is the target, but we may need to handle other categorical features if present (none in this case).\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Handle missing values by replacing with the mean (or other imputation methods)\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Check for outliers using boxplots\n",
    "sns.boxplot(x=df['Glucose'])\n",
    "plt.show()\n",
    "\n",
    "# Remove outliers (example: removing values beyond the 95th percentile)\n",
    "df = df[df['Glucose'] < df['Glucose'].quantile(0.95)]\n",
    "\n",
    "# Check the data types\n",
    "print(df.dtypes)\n",
    "\n",
    "# Ensure all variables are numeric (already true in this dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Split the dataset into training and test sets\n",
    "# We will split the dataset into a training set and a test set to train and evaluate the model. Using a random seed ensures reproducibility.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Train a decision tree model\n",
    "# We will use a decision tree algorithm such as ID3 or C4.5 (scikit-learnâ€™s DecisionTreeClassifier implements C4.5), and we will perform cross-validation to optimize the model's hyperparameters.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize a DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {'max_depth': [3, 5, 7, 10, None],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_dt = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Evaluate the performance of the decision tree\n",
    "After training the model, we evaluate its performance on the test set using metrics like accuracy, precision, recall, and F1 score. We will also use a confusion matrix and ROC curve to visualize the results.\n",
    "\n",
    "Code Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_dt.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print(f'ROC AUC: {roc_auc:.2f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Diabetic', 'Diabetic'], yticklabels=['Non-Diabetic', 'Diabetic'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, best_dt.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label='ROC Curve')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. Interpret the decision tree\n",
    "After training the decision tree, we can visualize the tree to understand the splits, branches, and leaves. We can also determine the most important features.\n",
    "\n",
    "Code Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Visualize the trained decision tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(best_dt, filled=True, feature_names=X.columns, class_names=['Non-Diabetic', 'Diabetic'], rounded=True)\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "importances = best_dt.feature_importances_\n",
    "feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
    "print(feature_importance.sort_values(by='Importance', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. Validate the decision tree model\n",
    "Finally, we can validate the model by applying it to new data or testing its robustness to changes in the dataset or environment. Sensitivity analysis can also be used to check how the model responds to small changes in input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity analysis: Try slightly modifying the test set\n",
    "X_test_modified = X_test.copy()\n",
    "X_test_modified['Glucose'] += 5  # Slight modification\n",
    "y_pred_modified = best_dt.predict(X_test_modified)\n",
    "\n",
    "# Evaluate the performance again\n",
    "accuracy_modified = accuracy_score(y_test, y_pred_modified)\n",
    "print(f'Accuracy on modified test set: {accuracy_modified:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Import the dataset and examine the variables\n",
    "# To start, we'll load the dataset, check for any missing values, and perform exploratory data analysis (EDA) to understand the variables. This will include summary statistics and visualizations of the data.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Display summary statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Visualize the distribution of each feature\n",
    "df.hist(figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the correlation between features\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the distribution of the target variable\n",
    "sns.countplot(x='Outcome', data=df)\n",
    "plt.title('Distribution of Diabetic and Non-Diabetic Patients')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Preprocess the data\n",
    "In this step, we handle any missing values, remove outliers, and prepare the data for training. The data is already numeric, so no need for dummy variables.\n",
    "\n",
    "Code Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values by replacing with the mean of each column (for simplicity)\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Check for outliers in the columns using boxplots\n",
    "sns.boxplot(x=df['Glucose'])\n",
    "plt.show()\n",
    "\n",
    "# Remove outliers beyond the 95th percentile\n",
    "df = df[df['Glucose'] < df['Glucose'].quantile(0.95)]\n",
    "\n",
    "# Check for data types\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Split the dataset into a training set and a test set\n",
    "We will now split the data into training and testing sets, with a specified random seed to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "# Split the data into a training set and a test set (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Train a decision tree model using cross-validation\n",
    "We will use a decision tree classifier from sklearn and optimize the hyperparameters using grid search with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize the decision tree classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define a parameter grid for hyperparameter tuning\n",
    "param_grid = {'max_depth': [3, 5, 7, 10, None],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# Set up grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters found by GridSearchCV\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_dt = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Evaluate the performance of the decision tree model\n",
    "After training the model, we will evaluate it on the test set using metrics such as accuracy, precision, recall, F1 score, and visualize the confusion matrix and ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_dt.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print(f'ROC AUC: {roc_auc:.2f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Diabetic', 'Diabetic'], yticklabels=['Non-Diabetic', 'Diabetic'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, best_dt.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label='ROC Curve')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. Interpret the decision tree\n",
    "Now that we have trained the model, we will examine the splits, branches, and leaves of the tree to understand how the model makes its predictions. This also helps identify the most important variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(best_dt, filled=True, feature_names=X.columns, class_names=['Non-Diabetic', 'Diabetic'], rounded=True)\n",
    "plt.title('Decision Tree Visualization')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "importances = best_dt.feature_importances_\n",
    "feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
    "print(feature_importance.sort_values(by='Importance', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. Validate the decision tree model\n",
    "Finally, we will validate the decision tree's robustness by applying it to new data or testing its sensitivity to changes in the dataset. This could involve testing the model's performance on slightly modified data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity analysis: Slightly modify the test set (e.g., add noise to features)\n",
    "X_test_modified = X_test.copy()\n",
    "X_test_modified['Glucose'] += 5  # Adding small noise to one feature\n",
    "\n",
    "# Predict on the modified test set\n",
    "y_pred_modified = best_dt.predict(X_test_modified)\n",
    "\n",
    "# Evaluate the performance again\n",
    "accuracy_modified = accuracy_score(y_test, y_pred_modified)\n",
    "print(f'Accuracy on modified test set: {accuracy_modified:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
